{"id":"b0199ceb-28b9-4fea-9dd9-4ae43766dbcc","data":{"nodes":[{"id":"YouTubeTranscriptExtractor-dn5IM","type":"genericNode","position":{"x":512.0553818697423,"y":726.7505166996021},"data":{"type":"YouTubeTranscriptExtractor","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\r\nfrom langflow.inputs import MessageTextInput, DropdownInput\r\nfrom langflow.template import Output\r\nfrom langflow.schema.message import Message\r\nfrom youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound\r\nimport re\r\n\r\nclass YouTubeTranscriptExtractor(Component):\r\n    display_name = \"YouTube Transcript Extractor\"\r\n    description = \"Extracts transcript from a YouTube video link.\"\r\n    icon = \"youtube\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"youtube_link\",\r\n            display_name=\"YouTube Link\",\r\n            info=\"Link to the YouTube video.\",\r\n        ),\r\n        DropdownInput(\r\n            name=\"language\",\r\n            display_name=\"Language\",\r\n            options=[\"en\", \"pt\", \"es\"],\r\n            info=\"Language for the transcript (en: English, pt: Brazilian Portuguese, es: Spanish).\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Transcript\", name=\"transcript\", method=\"transcript_response\"),\r\n    ]\r\n\r\n    def transcript_response(self) -> Message:\r\n        youtube_link = self.youtube_link\r\n        language = self.language\r\n\r\n        video_id = self.extract_video_id(youtube_link)\r\n        \r\n        if video_id:\r\n            try:\r\n                transcript = self.get_transcript(video_id, language)\r\n                transcript_text = \" \".join([item['text'] for item in transcript])\r\n                message = Message(text=transcript_text)\r\n                self.status = transcript_text\r\n                return message\r\n            except Exception as e:\r\n                error_message = f\"Error: {str(e)}\"\r\n                self.status = error_message\r\n                return Message(text=error_message)\r\n        else:\r\n            error_message = \"Invalid YouTube link\"\r\n            self.status = error_message\r\n            return Message(text=error_message)\r\n\r\n    @staticmethod\r\n    def extract_video_id(link: str) -> str:\r\n        # Extract the video ID from the YouTube link\r\n        pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*'\r\n        match = re.search(pattern, link)\r\n        return match.group(1) if match else None\r\n\r\n    def get_transcript(self, video_id: str, language: str):\r\n        try:\r\n            return YouTubeTranscriptApi.get_transcript(video_id, languages=[language])\r\n        except NoTranscriptFound:\r\n            return YouTubeTranscriptApi.get_transcript(video_id)  # Fallback to the default language if the specified language transcript is not found\r\n\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"language":{"trace_as_metadata":true,"options":["en","pt","es"],"required":false,"placeholder":"","show":true,"value":"pt","name":"language","display_name":"Language","advanced":false,"dynamic":false,"info":"Language for the transcript (en: English, pt: Brazilian Portuguese, es: Spanish).","title_case":false,"type":"str","load_from_db":false},"youtube_link":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"youtube_link","display_name":"YouTube Link","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Link to the YouTube video.","title_case":false,"type":"str"}},"description":"Extracts transcript from a YouTube video link.","icon":"youtube","base_classes":["Message"],"display_name":"Transcricao","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"transcript","display_name":"Transcript","method":"transcript_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["youtube_link","language"],"beta":false,"edited":true},"id":"YouTubeTranscriptExtractor-dn5IM","description":"Extracts transcript from a YouTube video link.","display_name":"Custom Component"},"selected":false,"width":384,"height":395},{"id":"Prompt-AHneK","type":"genericNode","position":{"x":971.0553818697423,"y":728.7505166996021},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"Transcrição: {transcrição}\n\nBaseado na transcrição acima, crie três títulos e 3 descrições envolventes e informativas para vídeos do YouTube. Cada título deve ser cativante e otimizado para mecanismos de busca, enquanto as descrições devem ser detalhadas, incluindo palavras-chave relevantes, um breve resumo do conteúdo do vídeo e um chamado para ação.\n\n","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt"},"transcrição":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"transcrição","display_name":"transcrição","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["transcrição"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-AHneK","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":423},{"id":"YouTubeTranscriptExtractor-WKo5q","type":"genericNode","position":{"x":509.4821516661267,"y":1384.8275679435299},"data":{"type":"YouTubeTranscriptExtractor","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\r\nfrom langflow.inputs import MessageTextInput, DropdownInput\r\nfrom langflow.template import Output\r\nfrom langflow.schema.message import Message\r\nfrom youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound\r\nimport re\r\n\r\nclass YouTubeTranscriptExtractor(Component):\r\n    display_name = \"YouTube Transcript Extractor\"\r\n    description = \"Extracts transcript from a YouTube video link.\"\r\n    icon = \"youtube\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"youtube_link\",\r\n            display_name=\"YouTube Link\",\r\n            info=\"Link to the YouTube video.\",\r\n        ),\r\n        DropdownInput(\r\n            name=\"language\",\r\n            display_name=\"Language\",\r\n            options=[\"en\", \"pt\", \"es\"],\r\n            info=\"Language for the transcript (en: English, pt: Brazilian Portuguese, es: Spanish).\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Transcript\", name=\"transcript\", method=\"transcript_response\"),\r\n    ]\r\n\r\n    def transcript_response(self) -> Message:\r\n        youtube_link = self.youtube_link\r\n        language = self.language\r\n\r\n        video_id = self.extract_video_id(youtube_link)\r\n        \r\n        if video_id:\r\n            try:\r\n                transcript = self.get_transcript(video_id, language)\r\n                transcript_text = self.format_transcript(transcript)\r\n                message = Message(text=transcript_text)\r\n                self.status = transcript_text\r\n                return message\r\n            except Exception as e:\r\n                error_message = f\"Error: {str(e)}\"\r\n                self.status = error_message\r\n                return Message(text=error_message)\r\n        else:\r\n            error_message = \"Invalid YouTube link\"\r\n            self.status = error_message\r\n            return Message(text=error_message)\r\n\r\n    @staticmethod\r\n    def extract_video_id(link: str) -> str:\r\n        # Extract the video ID from the YouTube link\r\n        pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*'\r\n        match = re.search(pattern, link)\r\n        return match.group(1) if match else None\r\n\r\n    def get_transcript(self, video_id: str, language: str):\r\n        try:\r\n            return YouTubeTranscriptApi.get_transcript(video_id, languages=[language])\r\n        except NoTranscriptFound:\r\n            return YouTubeTranscriptApi.get_transcript(video_id)  # Fallback to the default language if the specified language transcript is not found\r\n\r\n    def format_transcript(self, transcript):\r\n        # Format the transcript with timestamps\r\n        formatted_transcript = \"\"\r\n        for item in transcript:\r\n            start = item['start']\r\n            text = item['text']\r\n            formatted_transcript += f\"[{self.format_time(start)}] {text}\\n\"\r\n        return formatted_transcript\r\n\r\n    @staticmethod\r\n    def format_time(seconds: float) -> str:\r\n        # Convert seconds to hh:mm:ss format\r\n        hours = int(seconds // 3600)\r\n        minutes = int((seconds % 3600) // 60)\r\n        seconds = int(seconds % 60)\r\n        return f\"{hours:02}:{minutes:02}:{seconds:02}\"\r\n\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"language":{"trace_as_metadata":true,"options":["en","pt","es"],"required":false,"placeholder":"","show":true,"value":"pt","name":"language","display_name":"Language","advanced":false,"dynamic":false,"info":"Language for the transcript (en: English, pt: Brazilian Portuguese, es: Spanish).","title_case":false,"type":"str","load_from_db":false},"youtube_link":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"youtube_link","display_name":"YouTube Link","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Link to the YouTube video.","title_case":false,"type":"str"}},"description":"Extracts transcript from a YouTube video link.","icon":"youtube","base_classes":["Message"],"display_name":"Transcricao time","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"transcript","display_name":"Transcript","method":"transcript_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["youtube_link","language"],"beta":false,"edited":true},"id":"YouTubeTranscriptExtractor-WKo5q","description":"Extracts transcript from a YouTube video link.","display_name":"Transcricao"},"selected":false,"width":384,"height":395,"positionAbsolute":{"x":509.4821516661267,"y":1384.8275679435299},"dragging":false},{"id":"TokenCountComponent-jySLt","type":"genericNode","position":{"x":988.3656006741945,"y":2052.2749124110574},"data":{"type":"TokenCountComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\r\nfrom langflow.inputs import MessageTextInput\r\nfrom langflow.schema.message import Message\r\nfrom langflow.template import Output\r\nimport tiktoken\r\n\r\nclass TokenCountComponent(Component):\r\n    display_name = \"Token Count Component\"\r\n    description = \"Calculates the number of tokens in a given prompt.\"\r\n    icon = \"calculator\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"prompt\",\r\n            display_name=\"Prompt\",\r\n            info=\"The prompt text to count tokens for.\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Token Count\", name=\"token_count\", method=\"count_tokens\"),\r\n    ]\r\n\r\n    def count_tokens(self) -> Message:\r\n        prompt = self.prompt\r\n        tokenizer = tiktoken.get_encoding(\"gpt2\")  # You can use other models' encoding as needed\r\n        token_count = len(tokenizer.encode(prompt))\r\n        \r\n        self.status = f\"Token Count: {token_count}\"\r\n        return Message(text=f\"Total de tokens na transcricao com timestamps: {token_count}\")","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"prompt":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"prompt","display_name":"Prompt","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The prompt text to count tokens for.","title_case":false,"type":"str"}},"description":"Calculates the number of tokens in a given prompt.","icon":"calculator","base_classes":["Message"],"display_name":"Tokens","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"token_count","display_name":"Token Count","method":"count_tokens","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["prompt"],"beta":false,"edited":true},"id":"TokenCountComponent-jySLt","description":"Calculates the number of tokens in a given prompt.","display_name":"Tokens"},"selected":false,"width":384,"height":337},{"id":"Prompt-Q1dIh","type":"genericNode","position":{"x":968.2094098489727,"y":230.91601919349318},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"Transcrição:\n{transcrição}\n\nBaseado na transcrição anterior, elabore um prompt para geração de imagem utilizando a ferramenta DallE, sua resposta deve conter apenas o prompt e nada mais, nem uma palavra de introdução. O prompt deve ser em ingles.\n","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt"},"transcrição":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"transcrição","display_name":"transcrição","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["transcrição"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-Q1dIh","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":423},{"id":"Prompt-JN9hO","type":"genericNode","position":{"x":1951.4225261815905,"y":-28.748434870017945},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"Create an image with a minimalistic style, without any text with the following context: {context}","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt"},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-JN9hO","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":423},{"id":"DallEImageGenerator-xetXo","type":"genericNode","position":{"x":2464.8906908652652,"y":-29.897133225014727},"data":{"type":"DallEImageGenerator","node":{"template":{"_type":"Component","activate_image_generation":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"activate_image_generation","display_name":"Activate Image Generation","advanced":false,"dynamic":false,"info":"Toggle to activate or deactivate image generation.","title_case":false,"type":"bool"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\r\nfrom langflow.inputs import MessageTextInput, SecretStrInput, BoolInput\r\nfrom langflow.template import Output\r\nfrom langflow.schema.message import Message\r\nfrom openai import OpenAI\r\n\r\nclass DallEImageGenerator(Component):\r\n    display_name = \"Dall-E Image Generator\"\r\n    description = \"Generates an image using the OpenAI Dall-E model from a text prompt.\"\r\n    icon = \"image\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"prompt\",\r\n            display_name=\"Text Prompt\",\r\n            info=\"Enter the text prompt to generate an image.\",\r\n        ),\r\n        SecretStrInput(\r\n            name=\"openai_api_key\",\r\n            display_name=\"OpenAI API Key\",\r\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\r\n            advanced=False,\r\n            value=\"OPENAI_API_KEY\",\r\n        ),\r\n        BoolInput(\r\n            name=\"activate_image_generation\",\r\n            display_name=\"Activate Image Generation\",\r\n            info=\"Toggle to activate or deactivate image generation.\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Generated Image\", name=\"generated_image\", method=\"generate_image\"),\r\n    ]\r\n\r\n    def generate_image(self) -> Message:\r\n        if not self.activate_image_generation:\r\n            self.status = \"Image generation is deactivated.\"\r\n            return Message(text=\"Image generation is deactivated.\")\r\n\r\n        prompt = self.prompt\r\n        openai_api_key = self.openai_api_key\r\n        client = OpenAI(api_key=openai_api_key)\r\n\r\n        try:\r\n            response = client.images.generate(\r\n                model=\"dall-e-3\",\r\n                prompt=prompt,\r\n                size=\"1792x1024\",\r\n                quality=\"standard\",\r\n                n=1,\r\n            )\r\n\r\n            image_url = response.data[0].url\r\n            self.status = f\"{image_url}\"\r\n            return Message(text=image_url)\r\n\r\n        except Exception as e:\r\n            error_message = f\"Failed to generate image: {str(e)}\"\r\n            self.status = error_message\r\n            return Message(text=error_message)\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"openai_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str"},"prompt":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"prompt","display_name":"Text Prompt","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter the text prompt to generate an image.","title_case":false,"type":"str"}},"description":"Generates an image using the OpenAI Dall-E model from a text prompt.","icon":"image","base_classes":["Message"],"display_name":"DallE Image","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":true,"outputs":[{"types":["Message"],"selected":"Message","name":"generated_image","display_name":"Generated Image","method":"generate_image","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["prompt","openai_api_key","activate_image_generation"],"beta":false,"edited":true},"id":"DallEImageGenerator-xetXo","description":"Generates an image using the OpenAI Dall-E model from a text prompt.","display_name":"Custom Component"},"selected":false,"width":384,"height":507,"dragging":false},{"id":"ImageDisplayComponent-zr2SA","type":"genericNode","position":{"x":2968.0205703539673,"y":-35.640625},"data":{"type":"ImageDisplayComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\r\nfrom langflow.inputs import MessageTextInput\r\nfrom langflow.template import Output\r\nfrom langflow.schema import Data\r\nfrom langflow.schema.message import Message\r\nfrom typing import Optional\r\nimport requests\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport os\r\nimport mimetypes\r\nfrom datetime import datetime\r\n\r\nclass ImageDisplayComponent(Component):\r\n    display_name = \"Image Display\"\r\n    description = \"Displays an image from a given HTTP URL and saves it to the download folder.\"\r\n    icon = \"save\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"image_url\",\r\n            display_name=\"Image URL\",\r\n            info=\"URL of the image to display and save.\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Image Data\", name=\"image_data\", method=\"fetch_image\"),\r\n    ]\r\n\r\n    def fetch_image(self) -> Message:\r\n        image_url = self.image_url\r\n\r\n        try:\r\n            response = requests.get(image_url)\r\n            response.raise_for_status()  # Raise an error on a bad status code\r\n            image = Image.open(BytesIO(response.content))\r\n\r\n            # Display the image\r\n            image.show()\r\n\r\n            # Save the image to the download folder\r\n            download_folder = os.path.expanduser(\"~/Downloads\")\r\n            if not os.path.exists(download_folder):\r\n                os.makedirs(download_folder)\r\n\r\n            # Determine the file extension\r\n            file_extension = os.path.splitext(image_url)[-1]\r\n            if not file_extension:\r\n                content_type = response.headers['Content-Type']\r\n                file_extension = mimetypes.guess_extension(content_type)\r\n\r\n            if not file_extension:\r\n                file_extension = '.png'  # Default to PNG if the extension cannot be determined\r\n\r\n            # Create a filename based on the current date and time\r\n            current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\r\n            file_name = f\"image_{current_time}{file_extension}\"\r\n            file_path = os.path.join(download_folder, file_name)\r\n            \r\n            image.save(file_path)\r\n\r\n            result = f\"Image displayed and saved to {file_path} successfully.\"\r\n        except Exception as e:\r\n            result = f\"Error displaying or saving image: {e}\"\r\n\r\n        self.status = result\r\n        return Message(text=result)\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"image_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"image_url","display_name":"Image URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"URL of the image to display and save.","title_case":false,"type":"str"}},"description":"Displays an image from a given HTTP URL and saves it to the download folder.","icon":"save","base_classes":["Message"],"display_name":"Salva e mostra","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"image_data","display_name":"Image Data","method":"fetch_image","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["image_url"],"beta":false,"edited":false},"id":"ImageDisplayComponent-zr2SA","description":"Displays an image from a given HTTP URL and saves it to the download folder.","display_name":"Custom Component"},"selected":false,"width":384,"height":337,"dragging":false},{"id":"ChatInput-FNvsY","type":"genericNode","position":{"x":-222,"y":1042.1544190528641},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"files","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"https://youtu.be/1uOC5Va5-C4","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_message","display_name":"Store Messages","advanced":false,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","store_message","sender","sender_name","session_id","files"],"beta":false,"edited":false},"id":"ChatInput-FNvsY","description":"Get chat inputs from the Playground.","display_name":"Chat Input"},"selected":true,"width":384,"height":385,"positionAbsolute":{"x":-222,"y":1042.1544190528641},"dragging":false},{"id":"ChatOutput-8bkQA","type":"genericNode","position":{"x":2079.3477757698797,"y":1182.2467853293003},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-8bkQA","description":"Display a chat message in the Playground.","display_name":"Chat Output"},"selected":false,"width":384,"height":309},{"id":"ChatOutput-skbLA","type":"genericNode","position":{"x":2263.850650756554,"y":644.5883420622843},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-skbLA","description":"Display a chat message in the Playground.","display_name":"Chat Output"},"selected":false,"width":384,"height":307,"dragging":false,"positionAbsolute":{"x":2263.850650756554,"y":644.5883420622843}},{"id":"ChatOutput-CiOI4","type":"genericNode","position":{"x":1483.6626947534037,"y":2151.061310131487},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-CiOI4","description":"Display a chat message in the Playground.","display_name":"Chat Output"},"selected":false,"width":384,"height":309},{"id":"Prompt-j6tC7","type":"genericNode","position":{"x":1033.4182084839103,"y":1451.860875032592},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"Elabore uma lista de timestamps - formato HH:MM:SS (máximo de {max_timestamps}) timestamps com descrições bem resumidas com base nessa transcrição: \n{transcrição}","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt"},"max_timestamps":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"8","fileTypes":[],"file_path":"","password":false,"name":"max_timestamps","display_name":"max_timestamps","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"transcrição":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"transcrição","display_name":"transcrição","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["max_timestamps","transcrição"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-j6tC7","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":517},{"id":"ChatOutput-WYPNU","type":"genericNode","position":{"x":2053.3477757698797,"y":1820.2467853293},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-WYPNU","description":"Display a chat message in the Playground.","display_name":"Chat Output"},"selected":false,"width":384,"height":309},{"id":"OpenAIModel-Hxfea","type":"genericNode","position":{"x":1511.3133670810275,"y":768.4952670698779},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"required":false,"placeholder":"","show":true,"value":"gpt-3.5-turbo","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str"},"openai_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"0.3","name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","openai_api_key","temperature","stream","system_message","seed"],"beta":false,"edited":false},"id":"OpenAIModel-Hxfea"},"selected":false,"width":384,"height":623,"positionAbsolute":{"x":1511.3133670810275,"y":768.4952670698779},"dragging":false},{"id":"OpenAIModel-ukmrN","type":"genericNode","position":{"x":1529.9735468023673,"y":1434.8903917951552},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"required":false,"placeholder":"","show":true,"value":"gpt-3.5-turbo","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str"},"openai_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"0.3","name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","openai_api_key","temperature","stream","system_message","seed"],"beta":false,"edited":false},"id":"OpenAIModel-ukmrN"},"selected":false,"width":384,"height":623,"positionAbsolute":{"x":1529.9735468023673,"y":1434.8903917951552},"dragging":false},{"id":"OpenAIModel-cilxa","type":"genericNode","position":{"x":1470.7761084982303,"y":-36.34006017529413},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"required":false,"placeholder":"","show":true,"value":"gpt-3.5-turbo","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str"},"openai_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system_message":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"0.3","name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","openai_api_key","temperature","stream","system_message","seed"],"beta":false,"edited":false},"id":"OpenAIModel-cilxa"},"selected":false,"width":384,"height":623,"positionAbsolute":{"x":1470.7761084982303,"y":-36.34006017529413},"dragging":false}],"edges":[{"source":"YouTubeTranscriptExtractor-dn5IM","target":"Prompt-AHneK","sourceHandle":"{œdataTypeœ:œYouTubeTranscriptExtractorœ,œidœ:œYouTubeTranscriptExtractor-dn5IMœ,œnameœ:œtranscriptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œtranscriçãoœ,œidœ:œPrompt-AHneKœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-YouTubeTranscriptExtractor-dn5IM{œdataTypeœ:œYouTubeTranscriptExtractorœ,œidœ:œYouTubeTranscriptExtractor-dn5IMœ,œnameœ:œtranscriptœ,œoutput_typesœ:[œMessageœ]}-Prompt-AHneK{œfieldNameœ:œtranscriçãoœ,œidœ:œPrompt-AHneKœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"transcrição","id":"Prompt-AHneK","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"YouTubeTranscriptExtractor","id":"YouTubeTranscriptExtractor-dn5IM","name":"transcript","output_types":["Message"]}},"selected":false},{"source":"YouTubeTranscriptExtractor-WKo5q","target":"TokenCountComponent-jySLt","sourceHandle":"{œdataTypeœ:œYouTubeTranscriptExtractorœ,œidœ:œYouTubeTranscriptExtractor-WKo5qœ,œnameœ:œtranscriptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œpromptœ,œidœ:œTokenCountComponent-jySLtœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-YouTubeTranscriptExtractor-WKo5q{œdataTypeœ:œYouTubeTranscriptExtractorœ,œidœ:œYouTubeTranscriptExtractor-WKo5qœ,œnameœ:œtranscriptœ,œoutput_typesœ:[œMessageœ]}-TokenCountComponent-jySLt{œfieldNameœ:œpromptœ,œidœ:œTokenCountComponent-jySLtœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"prompt","id":"TokenCountComponent-jySLt","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"YouTubeTranscriptExtractor","id":"YouTubeTranscriptExtractor-WKo5q","name":"transcript","output_types":["Message"]}},"selected":false},{"source":"YouTubeTranscriptExtractor-dn5IM","target":"Prompt-Q1dIh","sourceHandle":"{œdataTypeœ:œYouTubeTranscriptExtractorœ,œidœ:œYouTubeTranscriptExtractor-dn5IMœ,œnameœ:œtranscriptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œtranscriçãoœ,œidœ:œPrompt-Q1dIhœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-YouTubeTranscriptExtractor-dn5IM{œdataTypeœ:œYouTubeTranscriptExtractorœ,œidœ:œYouTubeTranscriptExtractor-dn5IMœ,œnameœ:œtranscriptœ,œoutput_typesœ:[œMessageœ]}-Prompt-Q1dIh{œfieldNameœ:œtranscriçãoœ,œidœ:œPrompt-Q1dIhœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"transcrição","id":"Prompt-Q1dIh","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"YouTubeTranscriptExtractor","id":"YouTubeTranscriptExtractor-dn5IM","name":"transcript","output_types":["Message"]}},"selected":false},{"source":"ChatInput-FNvsY","target":"YouTubeTranscriptExtractor-WKo5q","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-FNvsYœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œyoutube_linkœ,œidœ:œYouTubeTranscriptExtractor-WKo5qœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-ChatInput-FNvsY{œdataTypeœ:œChatInputœ,œidœ:œChatInput-FNvsYœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-YouTubeTranscriptExtractor-WKo5q{œfieldNameœ:œyoutube_linkœ,œidœ:œYouTubeTranscriptExtractor-WKo5qœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"youtube_link","id":"YouTubeTranscriptExtractor-WKo5q","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-FNvsY","name":"message","output_types":["Message"]}},"selected":false},{"source":"TokenCountComponent-jySLt","target":"ChatOutput-CiOI4","sourceHandle":"{œdataTypeœ:œTokenCountComponentœ,œidœ:œTokenCountComponent-jySLtœ,œnameœ:œtoken_countœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-CiOI4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-TokenCountComponent-jySLt{œdataTypeœ:œTokenCountComponentœ,œidœ:œTokenCountComponent-jySLtœ,œnameœ:œtoken_countœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-CiOI4{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-CiOI4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-CiOI4","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TokenCountComponent","id":"TokenCountComponent-jySLt","name":"token_count","output_types":["Message"]}},"selected":false},{"source":"YouTubeTranscriptExtractor-WKo5q","target":"Prompt-j6tC7","sourceHandle":"{œdataTypeœ:œYouTubeTranscriptExtractorœ,œidœ:œYouTubeTranscriptExtractor-WKo5qœ,œnameœ:œtranscriptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œtranscriçãoœ,œidœ:œPrompt-j6tC7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-YouTubeTranscriptExtractor-WKo5q{œdataTypeœ:œYouTubeTranscriptExtractorœ,œidœ:œYouTubeTranscriptExtractor-WKo5qœ,œnameœ:œtranscriptœ,œoutput_typesœ:[œMessageœ]}-Prompt-j6tC7{œfieldNameœ:œtranscriçãoœ,œidœ:œPrompt-j6tC7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"transcrição","id":"Prompt-j6tC7","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"YouTubeTranscriptExtractor","id":"YouTubeTranscriptExtractor-WKo5q","name":"transcript","output_types":["Message"]}},"selected":false},{"source":"ChatInput-FNvsY","target":"YouTubeTranscriptExtractor-dn5IM","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-FNvsYœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œyoutube_linkœ,œidœ:œYouTubeTranscriptExtractor-dn5IMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-ChatInput-FNvsY{œdataTypeœ:œChatInputœ,œidœ:œChatInput-FNvsYœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-YouTubeTranscriptExtractor-dn5IM{œfieldNameœ:œyoutube_linkœ,œidœ:œYouTubeTranscriptExtractor-dn5IMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"youtube_link","id":"YouTubeTranscriptExtractor-dn5IM","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-FNvsY","name":"message","output_types":["Message"]}},"selected":false},{"source":"Prompt-AHneK","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-AHneKœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-Hxfea","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Hxfeaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-Hxfea","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-AHneK","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-AHneK{œdataTypeœ:œPromptœ,œidœ:œPrompt-AHneKœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-Hxfea{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Hxfeaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"OpenAIModel-Hxfea","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Hxfeaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-8bkQA","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-8bkQAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-8bkQA","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-Hxfea","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-Hxfea{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Hxfeaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-8bkQA{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-8bkQAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"Prompt-j6tC7","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-j6tC7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-ukmrN","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-ukmrNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-ukmrN","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-j6tC7","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-j6tC7{œdataTypeœ:œPromptœ,œidœ:œPrompt-j6tC7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-ukmrN{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-ukmrNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"OpenAIModel-ukmrN","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-ukmrNœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-WYPNU","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-WYPNUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-WYPNU","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-ukmrN","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-ukmrN{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-ukmrNœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-WYPNU{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-WYPNUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"Prompt-Q1dIh","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-Q1dIhœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-cilxa","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-cilxaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-cilxa","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-Q1dIh","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-Q1dIh{œdataTypeœ:œPromptœ,œidœ:œPrompt-Q1dIhœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-cilxa{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-cilxaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"OpenAIModel-cilxa","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-cilxaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-JN9hO","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-JN9hOœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-JN9hO","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-cilxa","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-cilxa{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-cilxaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-JN9hO{œfieldNameœ:œcontextœ,œidœ:œPrompt-JN9hOœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"},{"source":"Prompt-JN9hO","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-JN9hOœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-skbLA","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-skbLAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-skbLA","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-JN9hO","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-JN9hO{œdataTypeœ:œPromptœ,œidœ:œPrompt-JN9hOœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-skbLA{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-skbLAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"}],"viewport":{"x":224.04197541957456,"y":-600.9615306180358,"zoom":0.8705505632961248}},"description":"Create, Curate, Communicate with Langflow.","name":"Gerador_transcript_youtube","last_tested_version":"1.0.9","endpoint_name":null,"is_component":false}